# ğŸ“– PrivCL â€“ Privacy-Enhanced Federated Contrastive Learning

## ğŸ” Problem Statement
Federated Learning (FL) enables collaborative training without sharing raw data. However:
- **Privacy risks** remain due to gradient leakage and inference attacks.  
- **Non-IID data** across clients degrades representation quality.  
- Sensitive applications (e.g., healthcare, finance, IoT) require **formal privacy guarantees**.  
- Real-world datasets are **multimodal** (images, graphs, time-series), demanding advanced representation learning.  

**PrivCL** addresses these challenges by integrating:
- **Contrastive Learning (CL)** for robust representations.  
- **Graph-augmented and temporal-aware encoders** for multimodal signals.  
- **Adaptive Differential Privacy (DP)** to dynamically tune noise per client.  
- **Homomorphic Encryption (HE)** for secure aggregation at the server.  

Research Paper (Unpublished Draft)
This project was accompanied by a research study titled "PrivCL: Privacy-Enhanced Federated Contrastive Learning with Adaptive Differential Privacy and Homomorphic Encryption."

You can read the full draft here:
ğŸ‘‰ PrivCL_Research_Draft.pdf

Note: This paper is an unpublished academic draft submitted as coursework but demonstrates the full methodology, experiments, and theoretical background behind the project.

This notebook implements a research prototype of PrivCL, starting with federated CL on MNIST, then extending to graph, time-series, DP, and HE.


This notebook implements a **research prototype** of PrivCL, starting with federated CL on MNIST, then extending to **graph, time-series, DP, and HE**.  

---

## âš™ï¸ Key Features
- **Federated Contrastive Learning**
  - SimCLR-style NT-Xent loss.  
  - Non-IID partitioning via Dirichlet distribution.  
- **Encoders**
  - CNN (`SmallConvEncoder`, `ImageNet`) for images.  
  - GNN (`GraphEncoder`, `GraphNet`) for graph data.  
  - RNN (`TemporalEncoder`, `TimeNet`) for time-series.  
  - `FusionProjection`, `MultimodalEncoder` for multimodal learning.  
- **Clients**
  - `PrivCLClient` â€“ baseline FL+CL.  
  - `MultimodalPrivCLClient` â€“ multimodal learning.  
  - `AdaptiveDPPrivCLClient` â€“ DP-enabled FL.  
  - `HEClient` â€“ homomorphic encryption enabled FL.  
- **Server**
  - Standard FedAvg via [Flower](https://flower.dev).  
  - `SecureHEFedAvg` â€“ secure aggregation with HE.  
- **Synthetic Data Utilities**
  - `synth_graph` â†’ random graphs.  
  - `synth_timeseries` â†’ synthetic temporal signals.  
  - `MultimodalDataset` â†’ combined graph+time-series.  

---

## ğŸ“¦ Installation
```bash
# Core libraries
pip install torch torchvision flwr numpy tqdm

# Graph support
pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-2.0.0+cpu.html

# Time-series
pip install tslearn

# Privacy & Encryption
pip install opacus tenseal

FED_CL.ipynb
â”‚
â”œâ”€â”€ Data Utilities
â”‚   â”œâ”€â”€ TwoCropTransform, ContrastiveMNIST
â”‚   â”œâ”€â”€ partition_dirichlet (non-IID partitioning)
â”‚   â”œâ”€â”€ synth_graph, synth_timeseries, MultimodalDataset
â”‚
â”œâ”€â”€ Models
â”‚   â”œâ”€â”€ SmallConvEncoder, ImageNet (images)
â”‚   â”œâ”€â”€ GraphEncoder, GraphNet (graphs)
â”‚   â”œâ”€â”€ TemporalEncoder, TimeNet (time-series)
â”‚   â”œâ”€â”€ ProjectionHead, FusionProjection, MultimodalEncoder
â”‚
â”œâ”€â”€ Loss
â”‚   â”œâ”€â”€ NTXentLoss, nt_xent_loss (contrastive loss)
â”‚
â”œâ”€â”€ Federated Clients
â”‚   â”œâ”€â”€ PrivCLClient (baseline)
â”‚   â”œâ”€â”€ MultimodalPrivCLClient
â”‚   â”œâ”€â”€ AdaptiveDPPrivCLClient
â”‚   â”œâ”€â”€ HEClient
â”‚
â”œâ”€â”€ Federated Server
â”‚   â”œâ”€â”€ SecureHEFedAvg (HE aggregation)
â”‚   â”œâ”€â”€ client_fn, main_simulation, start_simulation
```
```bash
from FED_CL import main_simulation
main_simulation(num_clients=5, rounds=10, local_epochs=1, batch_size=128, alpha=0.5)

from FED_CL import start_simulation
start_simulation(num_clients=4, rounds=5, local_steps=20)

client = AdaptiveDPPrivCLClient(...)

strategy = SecureHEFedAvg()
```
ğŸ“Š Evaluation

Representation Quality: linear probing, clustering.

Privacy: (Îµ, Î´)-DP budgets, resistance to inference attacks.

Utility: accuracy on downstream tasks.

Overhead: communication/computation cost.

ğŸ”® Extensions

Replace MNIST with Cora (graphs) or UCR archive (time-series).

Scale clients (5 â†’ 100+) to test federated robustness.

Add attack simulations (membership inference, backdoors).

ğŸ“š References

Flower
 â€“ Federated learning framework.

Opacus
 â€“ Differential Privacy in PyTorch.

TenSEAL
 â€“ Homomorphic Encryption.

PyTorch Geometric
 â€“ Graph learning.

tslearn
 â€“ Time-series learning.

 
